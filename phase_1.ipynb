{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da05bf2d",
   "metadata": {},
   "source": [
    "PART ONE: PARSING THE NEWS HEADLINES\n",
    "\n",
    "\n",
    "    Objective:Find any city and/or country names mentioned in each of the news headlines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667e1d7e",
   "metadata": {},
   "source": [
    "WORKFLOW:\n",
    "\n",
    "Step One\n",
    "\n",
    "    reference necessary modules:numpy\\pandas\\unidecode\\geonamescache\\re\\time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f90ca7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from unidecode import unidecode\n",
    "import geonamescache\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd46ed1",
   "metadata": {},
   "source": [
    "Step Two\n",
    "\n",
    "    bringing in external data through geonamescache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "deb4dead",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc = geonamescache.GeonamesCache()\n",
    "countries_info = gc.get_countries()\n",
    "us_states_info = gc.get_us_states()\n",
    "us_counties_info = gc.get_us_counties()\n",
    "cities_info = gc.get_cities()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f4a3e4",
   "metadata": {},
   "source": [
    "Step Three\n",
    "    \n",
    "    create function1\n",
    "    function:get the longest string in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a95cad32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def longest_str(original_list):\n",
    "    if original_list:\n",
    "        result = original_list[0]\n",
    "        for i in original_list:\n",
    "            if len(i) > len(result):\n",
    "                result = i\n",
    "        return result\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5261a4b5",
   "metadata": {},
   "source": [
    "Step Four\n",
    "\n",
    "    create function2\n",
    "    function:find any country names mentioned in each of the news headlines through regular expressions \n",
    "    Notice:We entere the title and the official city name and return the value are already unicode.This function is called after match_city_country is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdfde9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_country_from_headline(headline, original_country_name):\n",
    "    matched_country = []\n",
    "    for country_entry_value in countries_info.values():\n",
    "        country_name = unidecode(country_entry_value['name']).strip()\n",
    "        compiled_normalized_name = re.compile(r'\\b' + re.escape(country_name) + r'\\b')\n",
    "        country_name_matches = compiled_normalized_name.findall(headline)\n",
    "        if country_name_matches:\n",
    "            matched_country.append(country_name_matches[0])\n",
    "    if matched_country:\n",
    "        return longest_str(matched_country)\n",
    "    else:\n",
    "        return original_country_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc457e47",
   "metadata": {},
   "source": [
    "Step Four\n",
    "\n",
    "    create function3\n",
    "    function:match cities by title and preliminary match countries\n",
    "    Notice:The title is passed in unicode and the return value is unicode.This function is called before the match_country_from_headline function  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "565cf8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_match_city = 0\n",
    "def match_city_country(headline):\n",
    "    global counter_match_city\n",
    "    counter_match_city += 1\n",
    "    print('begin to match headline {}: {}'.format(counter_match_city, headline))\n",
    "\n",
    "    matched_city = {}\n",
    "    for city_ID, city_entry_value in cities_info.items():\n",
    "        city_names = [unidecode(city_entry_value['name']).strip()]\n",
    "        for alternate in city_entry_value['alternatenames']:\n",
    "            striped_normalized_alternate = unidecode(alternate).strip()\n",
    "            if striped_normalized_alternate:\n",
    "                city_names.append(striped_normalized_alternate)\n",
    "        for name in city_names:\n",
    "            compiled_normalized_name = re.compile(r'\\b' + re.escape(name) + r'\\b')\n",
    "            city_name_matches = compiled_normalized_name.findall(headline)\n",
    "            if city_name_matches:\n",
    "                matched_city[city_ID] = [city_name_matches[0], city_names[0], len(city_entry_value['alternatenames']),\n",
    "                                         city_entry_value['population']]\n",
    "    if matched_city:\n",
    "        # First, find the longest match. If the longest match is of the same length, choose the official city name instead of the nickname. If still cannot choose, choose the city with more nicknames, and if still cannot choose, choose the city with more people, for the nickname number and population reflect city fame and reputation.  \n",
    "        # Find the longest match and the longest matching length. Remove the other match\n",
    "        max_length = 0\n",
    "        for skimmed_city_info in matched_city.values():\n",
    "            max_length = max(max_length, len(skimmed_city_info[0]))\n",
    "        has_formal_name = False\n",
    "        delete_ID = []\n",
    "        for city_ID, skimmed_city_info in matched_city.items():\n",
    "            if len(skimmed_city_info[0]) < max_length:\n",
    "                delete_ID.append(city_ID)\n",
    "            else:\n",
    "                if skimmed_city_info[0] == skimmed_city_info[1]:\n",
    "                    has_formal_name = True\n",
    "        for id in delete_ID:\n",
    "            matched_city.pop(id)\n",
    "        delete_ID.clear()\n",
    "\n",
    "        # If there are formal names, remove other informal ones\n",
    "        if has_formal_name:\n",
    "            for city_ID, skimmed_city_info in matched_city.items():\n",
    "                if skimmed_city_info[0] != skimmed_city_info[1]:\n",
    "                    delete_ID.append(city_ID)\n",
    "            for id in delete_ID:\n",
    "                matched_city.pop(id)\n",
    "            delete_ID.clear()\n",
    "\n",
    "        # Delete cities with a low number of nicknames. The nicknames can reflect a city's cultural influence  \n",
    "        max_num_alternate = 0\n",
    "        for skimmed_city_info in matched_city.values():\n",
    "            max_num_alternate = max(max_num_alternate, skimmed_city_info[2])\n",
    "        for city_ID, skimmed_city_info in matched_city.items():\n",
    "            if skimmed_city_info[2] < max_num_alternate:\n",
    "                delete_ID.append(city_ID)\n",
    "        for id in delete_ID:\n",
    "            matched_city.pop(id)\n",
    "        delete_ID.clear()\n",
    "\n",
    "        # Returns the largest population of the remaining matches. The population can also indicate the influence of the city  \n",
    "        max_population = 0\n",
    "        for skimmed_city_info in matched_city.values():\n",
    "            max_population = max(max_population, skimmed_city_info[3])\n",
    "        for city_ID, skimmed_city_info in matched_city.items():\n",
    "            if skimmed_city_info[3] == max_population:\n",
    "                country_name = countries_info[cities_info[city_ID]['countrycode']]['name']\n",
    "                return skimmed_city_info[1], unidecode(country_name).strip()\n",
    "\n",
    "    matched_us_state = []\n",
    "    for us_state_entry_value in us_states_info.values():\n",
    "        compiled_state_name = re.compile(r'\\b' + re.escape(us_state_entry_value['name']) + r'\\b')\n",
    "        state_name_matches = compiled_state_name.findall(headline)\n",
    "        if state_name_matches:\n",
    "            matched_us_state.append(state_name_matches[0])\n",
    "    if matched_us_state:\n",
    "        return longest_str(matched_us_state), 'United States'\n",
    "\n",
    "    # There are more us_counties with the same name, but it doesn't matter for the corresponding countries are all United States  \n",
    "    matched_us_county = []\n",
    "    for county_info in us_counties_info:\n",
    "        compiled_county_name = re.compile(r'\\b' + re.escape(county_info['name']) + r'\\b')\n",
    "        county_name_matches = compiled_county_name.findall(headline)\n",
    "        if county_name_matches:\n",
    "            matched_us_county.append(county_name_matches[0])\n",
    "    if matched_us_county:\n",
    "        return longest_str(matched_us_county), 'United States'\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92c0c6c",
   "metadata": {},
   "source": [
    "Step Five\n",
    "\n",
    "    create function4\n",
    "    function:put the extracted data into a pandas DataFrame with three columns: headline, city, country.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb792bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_extract_headline_data(start, end=None):\n",
    "    result = np.genfromtxt(\"database/headlines.txt\", 'str', delimiter='\\n', encoding='utf-8')\n",
    "    result = pd.DataFrame(result, columns=['headline'], dtype='str')\n",
    "    result.drop_duplicates(ignore_index=True, inplace=True)\n",
    "    if end is None:\n",
    "        result = result.iloc[start:].reset_index(drop=True)\n",
    "    else:\n",
    "        result = result.iloc[start:end].reset_index(drop=True)\n",
    "    result['headline'] = result['headline'].apply(unidecode)\n",
    "    num_headline = len(result)\n",
    "\n",
    "    result['city'], result['country'] = None, None\n",
    "    for i in range(num_headline):\n",
    "        result.loc[i, 'city'], result.loc[i, 'country'] = match_city_country(result.loc[i, 'headline'])\n",
    "        result.loc[i, 'country'] = match_country_from_headline(result.loc[i, 'headline'], result.loc[i, 'country'])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2439eb",
   "metadata": {},
   "source": [
    "Step Six\n",
    "\n",
    "    create function5\n",
    "    function:build the main function. Save the result as a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "622bf9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    result = read_and_extract_headline_data(0)\n",
    "    result.to_csv('database/extracted_data.csv', index=False, encoding='utf-8')\n",
    "\n",
    "    print(result.describe())\n",
    "    print(result.info())\n",
    "    print(result[result.isnull().T.any()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2411537e",
   "metadata": {},
   "source": [
    "Step Six\n",
    "\n",
    "    create function6\n",
    "    function:calculate the running time based on seconds alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48cac954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin to match headline 1: Zika Outbreak Hits Miami\n",
      "begin to match headline 2: Could Zika Reach New York City?\n",
      "begin to match headline 3: First Case of Zika in Miami Beach\n",
      "begin to match headline 4: Mystery Virus Spreads in Recife, Brazil\n",
      "begin to match headline 5: Dallas man comes down with case of Zika\n",
      "begin to match headline 6: Trinidad confirms first Zika case\n",
      "begin to match headline 7: Zika Concerns are Spreading in Houston\n",
      "begin to match headline 8: Geneve Scientists Battle to Find Cure\n",
      "begin to match headline 9: The CDC in Atlanta is Growing Worried\n",
      "begin to match headline 10: Zika Infested Monkeys in Sao Paulo\n",
      "begin to match headline 11: Brownsville teen contracts Zika virus\n",
      "begin to match headline 12: Mosquito control efforts in St. Louis take new tactics with Zika threat\n",
      "begin to match headline 13: San Juan reports 1st U.S. Zika-related death amid outbreak\n",
      "begin to match headline 14: Flu outbreak in Galveston, Texas\n",
      "begin to match headline 15: Zika alert a Manila now threatened\n",
      "begin to match headline 16: Zika afflicts 7 in Iloilo City\n",
      "begin to match headline 17: New Los Angeles Hairstyle goes Viral\n",
      "begin to match headline 18: Louisiana Zika cases up to 26\n",
      "begin to match headline 19: Orlando volunteers aid Zika research\n",
      "begin to match headline 20: Zika infects pregnant woman in Cebu\n",
      "begin to match headline 21: Chicago's First Zika Case Confirmed\n",
      "begin to match headline 22: Tampa Bay Area Zika Case Count Climbs\n",
      "begin to match headline 23: Bad Water Leads to Sickness in Flint, Michigan\n",
      "begin to match headline 24: Baltimore plans for Zika virus\n",
      "begin to match headline 25: London Health Unit Tracks Mad Cow Disease\n",
      "begin to match headline 26: Zika cases in Vietnam's Ho Chi Minh City surge\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m      2\u001b[0m     timestamp1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     timestamp2 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m总共用时 \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m 秒\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (timestamp2 \u001b[38;5;241m-\u001b[39m timestamp1))\n",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m----> 2\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mread_and_extract_headline_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     result\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatabase/extracted_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(result\u001b[38;5;241m.\u001b[39mdescribe())\n",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36mread_and_extract_headline_data\u001b[1;34m(start, end)\u001b[0m\n\u001b[0;32m     12\u001b[0m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcity\u001b[39m\u001b[38;5;124m'\u001b[39m], result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcountry\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_headline):\n\u001b[1;32m---> 14\u001b[0m     result\u001b[38;5;241m.\u001b[39mloc[i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcity\u001b[39m\u001b[38;5;124m'\u001b[39m], result\u001b[38;5;241m.\u001b[39mloc[i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcountry\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mmatch_city_country\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mheadline\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     result\u001b[38;5;241m.\u001b[39mloc[i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcountry\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m match_country_from_headline(result\u001b[38;5;241m.\u001b[39mloc[i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheadline\u001b[39m\u001b[38;5;124m'\u001b[39m], result\u001b[38;5;241m.\u001b[39mloc[i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcountry\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36mmatch_city_country\u001b[1;34m(headline)\u001b[0m\n\u001b[0;32m     13\u001b[0m         city_names\u001b[38;5;241m.\u001b[39mappend(striped_normalized_alternate)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m city_names:\n\u001b[1;32m---> 15\u001b[0m     compiled_normalized_name \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mescape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     city_name_matches \u001b[38;5;241m=\u001b[39m compiled_normalized_name\u001b[38;5;241m.\u001b[39mfindall(headline)\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m city_name_matches:\n",
      "File \u001b[1;32mE:\\python\\lib\\re.py:251\u001b[0m, in \u001b[0;36mcompile\u001b[1;34m(pattern, flags)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompile\u001b[39m(pattern, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompile a regular expression pattern, returning a Pattern object.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\python\\lib\\re.py:303\u001b[0m, in \u001b[0;36m_compile\u001b[1;34m(pattern, flags)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sre_compile\u001b[38;5;241m.\u001b[39misstring(pattern):\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst argument must be string or compiled pattern\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 303\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[43msre_compile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (flags \u001b[38;5;241m&\u001b[39m DEBUG):\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(_cache) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m _MAXCACHE:\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;66;03m# Drop the oldest item\u001b[39;00m\n",
      "File \u001b[1;32mE:\\python\\lib\\sre_compile.py:764\u001b[0m, in \u001b[0;36mcompile\u001b[1;34m(p, flags)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m isstring(p):\n\u001b[0;32m    763\u001b[0m     pattern \u001b[38;5;241m=\u001b[39m p\n\u001b[1;32m--> 764\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[43msre_parse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    766\u001b[0m     pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mE:\\python\\lib\\sre_parse.py:948\u001b[0m, in \u001b[0;36mparse\u001b[1;34m(str, flags, state)\u001b[0m\n\u001b[0;32m    945\u001b[0m state\u001b[38;5;241m.\u001b[39mstr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m\n\u001b[0;32m    947\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 948\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[43m_parse_sub\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mSRE_FLAG_VERBOSE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    949\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Verbose:\n\u001b[0;32m    950\u001b[0m     \u001b[38;5;66;03m# the VERBOSE flag was switched on inside the pattern.  to be\u001b[39;00m\n\u001b[0;32m    951\u001b[0m     \u001b[38;5;66;03m# on the safe side, we'll parse the whole thing again...\u001b[39;00m\n\u001b[0;32m    952\u001b[0m     state \u001b[38;5;241m=\u001b[39m State()\n",
      "File \u001b[1;32mE:\\python\\lib\\sre_parse.py:441\u001b[0m, in \u001b[0;36m_parse_sub\u001b[1;34m(source, state, verbose, nested)\u001b[0m\n\u001b[0;32m    439\u001b[0m itemsappend \u001b[38;5;241m=\u001b[39m items\u001b[38;5;241m.\u001b[39mappend\n\u001b[0;32m    440\u001b[0m sourcematch \u001b[38;5;241m=\u001b[39m source\u001b[38;5;241m.\u001b[39mmatch\n\u001b[1;32m--> 441\u001b[0m start \u001b[38;5;241m=\u001b[39m \u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtell\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    443\u001b[0m     itemsappend(_parse(source, state, verbose, nested \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    444\u001b[0m                        \u001b[38;5;129;01mnot\u001b[39;00m nested \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m items))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    timestamp1 = time.time()\n",
    "    main()\n",
    "    timestamp2 = time.time()\n",
    "    print(\"总共用时 %f 秒\" % (timestamp2 - timestamp1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
